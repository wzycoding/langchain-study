当我们在跟大语言模型交流时，会发现大语言模型只能回答训练数据范围内的事情，如果你问它一些超出训练数据之外的事情，AI的回答往往就是胡言乱语，或者给出错误的答案。

那么我们不禁会想到：大模型拥有如此强大的能力，是否能让大模型掌握一些特定领域的知识，甚至接入企业内部的知识库数据？来帮助我们实现业务需求。

这就是本文要介绍的技术——**RAG（Retrieval Augmented Generation）即检索增强生成**
，本文将重点介绍RAG是什么以及使用LangChain实现RAG所需的关键组件，并分析它们之间的协作机制。具体实现细节将在后续文章中展开。

> 文中所有示例代码：https://github.com/wzycoding/langchain-study

## 一、什么是RAG

**RAG（Retrieval Augmented Generation）检索增强生成**
，RAG是一种将信息检索和大语言模型文本生成相结合的技术。简单来说，RAG就是在大语言模型生成答案之前，AI应用先从外部知识库中检索与用户提问相关的信息，然后将检索到的信息作为上下文通过提示词一起传递给大语言模型，从而让大语言模型根据检索出来的内容进行回答。

RAG解决了如下几个问题：

（1）**知识边界问题**：大语言模型所掌握的知识来源于训练数据，对超过自身训练数据以外的问题无法回答。

（2）**知识更新问题**：大语言模型训练完成后，除了重新训练之外无法继续获取最新的信息。

（3）**幻觉问题**：当大语言模型不确定答案时，编造一些错误信息，这种现象被称为幻觉。

（4）**回答特定领域专业问题**：目前参数比较大的大语言模型多数都是通用的模型，这些通用模型的知识面非常广，但是对特定领域知识了解的深度更低。

通过RAG技术就能在一定程度上解决上述的问题，让大语言模型拥有了获取特定领域知识的能力，并且给出准确、专业的回答。